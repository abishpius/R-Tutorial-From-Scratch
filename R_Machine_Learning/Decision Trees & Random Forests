#!/usr/bin/env Rscript

#Load the Data
library(tidyverse)
library(caret)
library(dslabs)
data("tissue_gene_expression")

#Use the rpart() function to fit a classification tree to the tissue_gene_expression dataset. Use the train() function to estimate the accuracy. Try out cp values of seq(0, 0.1, 0.01). Plot the accuracies to report the results of the best model. Which value of cp gives the highest accuracy?

fit <- with(tissue_gene_expression, 
                train(x, y, method = "rpart",
                      tuneGrid = data.frame(cp = seq(0, 0.1, 0.01))))
    
ggplot(fit)

# Note that there are only 6 placentas in the dataset. By default, rpart() requires 20 observations before splitting a node. That means that it is difficult to have a node in which placentas are the majority. Rerun the analysis you did in the exercise in Q1, but this time, allow rpart() to split any node. What is the accuracy now?

library(rpart)
data("tissue_gene_expression")
    
fit_rpart <- with(tissue_gene_expression, 
                      train(x, y, method = "rpart",
                            tuneGrid = data.frame(cp = seq(0, 0.10, 0.01)),
                            control = rpart.control(minsplit = 0)))
ggplot(fit_rpart)
confusionMatrix(fit_rpart)

#Which gene is at the first split?
plot(fit_rpart$finalModel)
text(fit_rpart$finalModel)


#We can see that with just seven genes, we are able to predict the tissue type. Now let's see if we can predict the tissue type with even fewer genes using a Random Forest. Use the train() function and the rf method to train a Random Forest model and save it to an object called fit. Try out values of mtry ranging from seq(50, 200, 25) (you can also explore other values on your own). What mtry value maximizes accuracy?
library(randomForest)
fit <- with(tissue_gene_expression, 
                train(x, y, method = "rf", 
                      nodesize = 1,
                      tuneGrid = data.frame(mtry = seq(50, 200, 25))))
    
ggplot(fit)

#Find the Importance of leaves
varImp(fit)

# The rpart() model we ran above produced a tree that used just seven predictors. Extracting the predictor names is not straightforward, but can be done. If the output of the call to train was fit_rpart, we can extract the names like this:

tree_terms <- as.character(unique(fit_rpart$finalModel$frame$var[!(fit_rpart$finalModel$frame$var == "<leaf>")]))
tree_terms

#The following code can be used to calculate the rank and importance in the Random Forest call for the predictors from the rpart() model:
data_frame(term = rownames(imp$importance), 
			importance = imp$importance$Overall) %>%
	mutate(rank = rank(-importance)) %>% arrange(desc(importance)) %>%
	filter(term %in% tree_terms)
